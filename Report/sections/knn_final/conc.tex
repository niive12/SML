\subsection{Conclusion}



Multiple test were performed to find the best possible way to optimize the time taken to run and the success gained from using the K-NN algorithm.
It was chosen to use 40 PC to optimize speed, while keeping a descend success rate.
Furthermore it was found to benefit the success rate when smoothing the data using a Gaussian filter with sigma of 0.9 and a filter size of 5.
The success rate was also improved by applying z-score normalization.

From the final test it could be concluded that the K-NN algorithm, with the found preprocessing scheme, was 85\% and 71\% accurate in its classifications for the easy and hard problem respectively.
Furthermore then it was found that the main error originates from the obvious similarities between '3' and '5' being classified as '8' and the '4' as '9'.
This errors accounted combine for more than 1/6 of the error in the two classification tasks.

Furthermore, the worst person was found to be G2M1.