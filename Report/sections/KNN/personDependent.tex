%1.5.1
%50/50, k = 10 -> tid vs DPI!
%confusion matrix pÃ¥ 1 run (100, 200 og 300 DPI) 
%1.5.2
%k vs Train Size (10 runs)
%1.5.3
%cross val. 10 runs 90/10 -> mean + var


\subsection{Single Person Tests}
This section tests the K-NN algorithm using data from one single person, namely Group three member two's data.
%
%The tests are performed first with the data split equally into a 50\% for the training and 50\% for the test set and the runtime is measured and compared for the three different image qualities. 
%
%Then test are performed with changing values of k and the training set size.
%
%Afterwards with a 90/10\% split for training and test respectively is computed and mean and variance is given.
%
%All the tests will be conducted using cross validation with 10 runs.

\subsubsection{Equally Sized training and test set}
To test the algorithm the data from the three different DPI's where split 50/50\% into a training and test set. 
The time to compute the test set for each DPI was then recorded and plotted. This is seen in figure \ref{fig:PersonDependent_5050}.
As expected does this scale with number of pixels. 

% % % % generated by timing.R
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{graphics/time_vs_dpi}
\caption{Timing of running a result with a 50/50\% split of group three member two's data}
\label{fig:PersonDependent_5050}
\end{figure}

Running the three test sets resulted in the confusion matrices seen in table \ref{tb:confus}.
They are all tested with a K of 10 and a split of 50/50\%.
In these it is seen which numbers do well and which numbers are hard to detect and which number they are detected as.
While the chance of matching a 1 with a 1 is really high there seem to be a high chance of getting a false positive as only 200 out of the 440 detected ones actually were correct.
And while the overall detection is good, the 8 is hard to detect with only 57\% success rate on 300 dpi.

% % % % generated by confusion.R
\begin{table}
    \centering
    \begin{subtable}{0.5\textwidth}
        \flushright
        \begin{tikzpicture}
            \node at (0,0) {};
            \node at (1,0) {\huge Correct number}; 
        \end{tikzpicture}
    \end{subtable}

    \begin{subtable}{0.1\textwidth}
        \flushright
        \begin{tikzpicture}
            \node[rotate=90] {\huge Guessed number};
        \end{tikzpicture}
    \end{subtable}
    \begin{subtable}{0.6\textwidth}
        \begin{subtable}{\textwidth}
            \centering
            {\scriptsize
                \begin{tabular}{l|*{10}{c}}
                    \input{graphics/confusion_100_1}
                \end{tabular}
            }
            \caption{Results for 100 DPI}
        \end{subtable}
        \begin{subtable}{\textwidth}
            \centering
            {\scriptsize
                \begin{tabular}{l|*{10}{c}}
                    \input{graphics/confusion_200_1}
                \end{tabular}
            }
            \caption{Results for 200 DPI}
        \end{subtable}
        \begin{subtable}{\textwidth}
            \centering
            {\scriptsize
                \begin{tabular}{l|*{10}{c}}
                    \input{graphics/confusion_300_1}
                \end{tabular}
            }
            \caption{Results for 300 DPI}
        \end{subtable}
    \end{subtable}
    \caption{Confusion matrices with different scaling}
    \label{tb:confus}
\end{table}

\subsubsection{Changing K and Training Set Size}
Tests was performed to see how both the value of K and size of the training set affects the accuracy of the K-NN algorithm. 
The result is given in figure \ref{fig:personDependent_contour}.

% \begin{figure}[H]
% \centering
% \includegraphics[width = 13cm]{graphics/graph_G3M2_20}
% \caption{Success Rate of the K-NN algorithm for changing values of K and the training set size.}
% \label{fig:personDependent_contour}
% \end{figure}

\begin{figure}[H]
\centering
\includegraphics[width = 13cm]{graphics/graph_G3M2_10_10}
\caption{Success Rate of the K-NN algorithm for changing values of K and the training set size.}
\label{fig:personDependent_contour}
\end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width = 13cm]{graphics/graph_G3M1_10_10}
% \caption{Success Rate of the K-NN algorithm for changing values of K and the training set size.}
% \label{fig:personDependent_contour}
% \end{figure}

The test conducted in figure \ref{fig:personDependent_contour} where done with a test size of 40 and run 10 times with cross validation.

As seen on figure \ref{fig:personDependent_contour}, then the accuracy of the algorithm appears to increase as the train set size increases.
Furthermore the optimal value of K is dependent on the size of training set. 
As the size of the training set increases, so does the area where a the best performance is obtained.
From figure \ref{fig:personDependent_contour} it can be seen that the results are best when using a training size which is above 300 and K ranging between 3 and 7.


\subsubsection{90/10 Data Split}
A cross validation was also carried out using a 90/10\% and 50/50\% split of the data. 
Each test was made on both group members to see the difference.
The result of this is seen in figure \ref{fig:PersonDependent_9010}.
The values can be seen in \ref{tb:cross}

% % % % Generating this with cross_test.R tonight
\begin{figure}[h]
\centering
% \includegraphics[width=\textwidth]{graphics/cross_test}
\caption[Cross validation]{Test result with a 90/10\% and 50/50\% split. The data points are taken as the mean of 10 runs using cross validation.}
\label{fig:PersonDependent_9010}
\end{figure}

\begin{table}[h]
\centering
    \begin{subtable}[b]{0.56\textwidth}
    \centering
        \begin{tabular}{lcccccc}
%             \input{graphics/cross_mean}
        \end{tabular}
        \caption{Mean success rate}
    \end{subtable}
    \begin{subtable}[b]{0.42\textwidth}
    \centering
        \begin{tabular}{lcccccc}
%             \input{graphics/cross_var}
        \end{tabular}
        \caption{Variance in success rate}
    \end{subtable}
    \caption[Success of functions]{Success mean and variance of different split and data sets.}
    \label{tb:cross}
\end{table}



